{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayes in Laplace\n",
    "\n",
    "Given the random variable $x$ obey standard normal distribution，$y=\\frac{1}{1+e^{-(Xw^T)}}$, which is the real distribution of $y$\n",
    "\n",
    "##   generation of data\n",
    "\n",
    "###  generation of  **$x$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改：将原来的都服从相同的正态分布改成服从均值为0，方差为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########构造x#################\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "def gen_x(var,n_samples,p_samples):\n",
    "    np.random.seed(9)####设置随机种子\n",
    "    cov = np.identity(p_samples)*var\n",
    "    #print(cov)####输出生成的协方差矩阵\n",
    "    center =np.zeros(10)\n",
    "    #print(center)###输出生成的均值array\n",
    "   # global data_x\n",
    "    data_x = multivariate_normal.rvs(mean=center, cov=cov, size=n_samples)\n",
    "    return (data_x)\n",
    "data_x = gen_x(2,1000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generation of  $w$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "######构造w和y#######\n",
    "def gen_w_y(sigma,center,p_samples,data_x):\n",
    "    w = np.random.normal(center, sigma, p_samples)\n",
    "    data_y= 1/(1+np.exp(-np.dot(data_x,w)))\n",
    "    #print(data_y[1:10])\n",
    "    y_prob = data_y\n",
    "    #maxprob = max(y_prob)\n",
    "    y = np.zeros(1000)\n",
    "    for i in range(y_prob.shape[0]):\n",
    "        if (y_prob[i]>0.5):\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0 \n",
    "    return w,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the *coef_* (the $\\hat{w}$) with logistic regression model from sklearn keeps away from the real $w$ , what we need to do is to get the distribution $p(w|X,y)$. \n",
    "According to the Gaussian mixture model(GMM), which can approximate any distribution. Let the $ g(w;\\mu,\\Sigma) \\approx p(w|X,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大量的模型种存在着各种未知的参数，传统的机器学习方法是用自变量和因变量之间的真实联系转换为数学关系（模型），求得参数。\n",
    "贝叶斯采用的是概率的思想，以现有的样本来推测最大可能下的参数值。$p(w|X,y)$就成为了重点关注对象。如何找到一组$w$使得$p(w|X,y)$最小。\n",
    "\n",
    "- 首先根据高斯混合模型可以知道，任何分布都可以用几个高斯分布叠加而成，因此虽然不知道$p(w|X,y)$的具体表达式，但是可以用一个高斯混合分布来近似表示它$g(w;\\mu,\\Sigma)$\n",
    "- 对$log(g(w;\\mu,\\Sigma))$进行泰勒展开(在一阶导为0处展开)，得到最后表达式，minimize该表达式时发现所需要的$w$就是在一阶导为0的$w$，它只是要求高斯分布的mode和目标分布的mode位置相同，方法就是把目标分布在mode处做泰勒级数展开到第二阶，然后用对应的高斯分布去代替，就是把未知系数给凑出来\n",
    "- 因为$log(g(w;\\mu,\\Sigma))$近似于$log(p(w|X,y))$，所以可以根据最基本的贝叶斯公式将$log(g(w;\\mu,\\Sigma))$的表达式给写出，以此计算$w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback–Leibler divergence(K.L.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "KL = E[log\\frac{g(w;\\mu,\\Sigma)}{p(w|X,y)}] \n",
    "= E[log\\frac{g(w;\\mu,\\Sigma)}{\\frac{p(y|X,w)p(w)}{p(y|x)}}]\n",
    "= E[log\\frac{g(w;\\mu,\\Sigma)}{p(y|X,w)p(w)}+logp(y|X)]\n",
    "= E[logg(w;\\mu,\\Sigma) - logp(y|X,w) - logp(w)] +logp(y|X)\n",
    "$$\n",
    "\n",
    "Let the $J=E[logg(w;\\mu,\\Sigma) - logp(y|X,w) - logp(w)]$, what we need to do is to minimize the $J$\n",
    "\n",
    "$$\n",
    "E(logp(y|X,w)) = log[p(y_1|X,w)p(y_2|X,w)p(y_3|X,w)\\cdots p(y_N|X,w)] = \\sum_{n=1}^N log p(y_n|X,w)=\\sum_{n=1}^N log \\frac{1}{1+e^{-X^Tw}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace approximate\n",
    "\n",
    "$$\n",
    "log (g(w;\\mu,\\Sigma)) \\propto log (g(\\hat{w};\\mu,\\Sigma))+ \\frac{\\delta log^{'}(g(w;\\mu,\\Sigma))}{\\delta w}|_{w=\\hat{w}} \\frac{(w-\\hat{w})}{1!}\n",
    "+ \\frac{\\delta log^{''}(g(w;\\mu,\\Sigma))}{\\delta w}|_{w=\\hat{w}} \\frac{(w-\\hat{w})^2}{2!}\n",
    "$$\n",
    "\n",
    "when $w = \\hat{w}$ , the First derivative is 0. $\\hat{w}$ is the value of Newton's method, So:\n",
    "\n",
    "$$\n",
    "g(w;\\mu,\\Sigma) \\propto log (g(\\hat{w};\\mu,\\Sigma))+\\frac{\\delta log^{''}(g(w;\\mu,\\Sigma))}{\\delta w}|_{w=\\hat{w}} \\frac{(w-\\hat{w})^2}{2!}\n",
    "$$\n",
    "\n",
    "Let $K = g(\\hat{w};\\mu,\\Sigma), \\mu = \\hat{w}, \\sigma^2 = \\frac{1}{v}, v =-\\frac{\\delta log^{''}(g(w;\\mu,\\Sigma))}{\\delta w}|_{w=\\hat{w}}$\n",
    "\n",
    "the g is transformed into :\n",
    "$$\n",
    "log (g(w;\\mu,\\Sigma)) \\propto logK - \\frac{(w-\\mu)^2}{2\\sigma^2 }\n",
    "$$\n",
    "As a result, the next step is to solve the $\\hat{w}$\n",
    "\n",
    "### $\\hat{w}$ of solution\n",
    "As we all know , $p(w|x,y) = \\frac{p(y|x,w)p(w)}{p(y|x)}$,then define the $log[g(w;x,y,\\sigma^2)] = log[p(y|x,w)p(w|\\sigma^2)]$. Obviously, the $\\frac{log[g(w;x,y,\\sigma^2)]}{p(w|x,y)} = C$, the $C$ is a constant, so if the maxmum of $log(g)$ is solved, the $\\hat{w}$ is what we need.\n",
    "\n",
    "- **First, we need to transform the function $log(g)$:**\n",
    "\n",
    "$$\n",
    "log[g(w;x,y,\\sigma^2)] = log[p(y|x,w)]+log[p(w|\\sigma^2)]=log[p(y=y_1|x_1,w)\\cdotp(y=y_2|x_2,w)\\cdots p(y=y_n|x_n,w)]+log[p(w|\\sigma^2)]=\\sum_{n=1}^N log[p(y = y_n|x_n,w)]+log[p(w|\\sigma^2)]=\\sum_{n=1}^N log[(\\frac{1}{1+exp(-w^Tx_n)})^{y_n}\\cdot (\\frac{exp(-w^Tx_n)}{1+exp(-w^Tx_n)})^{1-y_n}]+log[p(w|\\sigma^2)]\n",
    "$$\n",
    "\n",
    "- **Let $P_n = P(y_n = 1|w,x_n)$, the formula can be transformed as follows:**\n",
    "\n",
    "$$\n",
    "log[g(w;x,y,\\sigma^2)] =log[p(w|\\sigma^2)] + \\sum_{n=1}^Nlog [P_n^{y_n}\\cdot(1-P_n)^{1-y_n}] = log[p(w|\\sigma^2)] + \\sum_{n=1}^N({y_n}\\cdot log P_n+ (1-y_n)\\cdot log(1-P_n))\n",
    "$$\n",
    "\n",
    "- **$D$ is defined as the dimension of $w$, the formula can be transformed as follows:（注意：这里的w是表示向量）**\n",
    "$$\n",
    "log[g(w;x,y,\\sigma^2)] = -\\frac{D}{2}log2\\pi-Dlog\\sigma - \\frac{1}{2\\sigma^2}w^Tw+ \\sum_{n=1}^N({y_n}\\cdot log P_n+ (1-y_n)\\cdot log(1-P_n))\n",
    "$$\n",
    "- **the next step is to solve for the first derivative，注意这里是对所有的w求导**:\n",
    "$$\n",
    "\\frac{\\delta P_n}{\\delta w}= P_n(1-P_n)x_n\n",
    "$$\n",
    "$$\n",
    "\\frac{\\delta log[g(w;x,y,\\sigma^2)]}{\\delta w}= - \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N(\\frac{y_n}{P_n}\\frac{\\delta P_n}{\\delta w} - \\frac{1-y_n}{1-P_n}\\frac{\\delta P_n}{\\delta w})=- \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N x_n(y_n-P_n)\n",
    "\\space\n",
    "$$\n",
    "- **the next step is to solve for the second derivative**:\n",
    "$$\n",
    "\\frac{\\delta^2log[g(w;x,y,\\sigma^2)]}{\\delta w^2}= - \\frac{1}{\\sigma^2}-\\sum_{n=1}^N(x_n^2P_n(1-P_n))<0\n",
    "$$\n",
    "Because the second derivative is less than 0, the function must have a maximum. As a result, Newton method is used to solve the zero of the first derivative. Assume that $f(w) = \\frac{\\delta log[g(w;x,y,\\sigma^2)]}{\\delta w}$, $f^{(2)}(w) = \\frac{\\delta^2log[g(w;x,y,\\sigma^2)]}{\\delta w^2}$,\n",
    "$$\n",
    "w_{n+1} = w_n - \\frac{f(w_n)}{f^{(1)}(w_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "$$\n",
    "\\frac{\\delta log[g(w;x,y,\\sigma^2)]}{\\delta w}= - \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N(\\frac{y_n}{P_n}\\frac{\\delta P_n}{\\delta w} - \\frac{1-y_n}{1-P_n}\\frac{\\delta P_n}{\\delta w})=- \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N x_n(y_n-P_n)\n",
    "$$\n",
    "$$\n",
    "w_{n+1} = w_n - \\eta{f^{'}(w_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#求一阶导\n",
    "def delta1(w,sigma):   #w代表初始的随机生成的w,var代表先验的方差\n",
    "    initial_w = w\n",
    "    derivative1 = -(1/(sigma**2))*initial_w\n",
    "    temp = np.zeros(data_x.shape)\n",
    "    for i in range(0,data_x.shape[0]):\n",
    "        temp[i,] = data_x[i,]*(y[i]-1/(1+np.exp(-np.dot(data_x[i,],initial_w))))\n",
    "    derivative1 = temp.sum(axis = 0)+derivative1\n",
    "    return (derivative1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#求二阶导\n",
    "def delta2(w,sigma):\n",
    "    initial_w = w\n",
    "    derivative2 = -(1/(sigma**2))\n",
    "    temp = np.zeros(data_x.shape)\n",
    "    for i in range(0,data_x.shape[0]):\n",
    "        temp[i,] = data_x[i,]*data_x[i,]*(1-1/(1+np.exp(-np.dot(data_x[i,],initial_w))))*1/(1+np.exp(-np.dot(data_x[i,],initial_w)))\n",
    "    derivative2 = temp.sum(axis = 0)-derivative2\n",
    "    return(derivative2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算目标函数的值\n",
    "def cal(w,sigma):\n",
    "    D = w.shape[0]\n",
    "    tempsum = 0\n",
    "    for i in range(data_x.shape[0]):\n",
    "        p = 1/(1+np.exp(-np.dot(data_x[i,],w)))\n",
    "        tempsum = tempsum + y[i] * np.log(p) + (1-y[i]) * (1-np.log(p))\n",
    "    res = tempsum- (D/2)*np.log(2*np.pi) - D * np.log(sigma) - 1/(2*(sigma**2))*np.dot(w.T,w)\n",
    "    #print(tempsum-res)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "log[g(w;x,y,\\sigma^2)] = -\\frac{D}{2}log2\\pi-Dlog\\sigma - \\frac{1}{2\\sigma^2}w^Tw+ \\sum_{n=1}^N({y_n}\\cdot log P_n+ (1-y_n)\\cdot log(1-P_n))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "$$\n",
    "\\frac{\\delta log[g(w;x,y,\\sigma^2)]}{\\delta w}= - \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N(\\frac{y_n}{P_n}\\frac{\\delta P_n}{\\delta w} - \\frac{1-y_n}{1-P_n}\\frac{\\delta P_n}{\\delta w})=- \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N x_n(y_n-P_n)\n",
    "$$\n",
    "$$\n",
    "w_{n+1} = w_n - \\eta{f^{'}(w_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用上述函数求得目标函数一阶导为0的点\n",
    "if __name__ == '__main__':\n",
    "    eta = 0.01####学习率\n",
    "    p = 10 #生成w的维度\n",
    "    np.random.seed(0)\n",
    "    sigma0 = 1 #设置w的先验参数信息\n",
    "    initial_w = np.random.normal(1, sigma0, p)####利用w的先验随机生成w，此处均值为0\n",
    "    w0 = initial_w\n",
    "    epsilon = 1\n",
    "    i = 0\n",
    "    rec = []\n",
    "    while(epsilon>0.001): #梯度下降\n",
    "        w1 = w0 + eta * delta1(w0,sigma0)\n",
    "        epsilon = abs(cal(w1,sigma0) - cal(w0,sigma0))\n",
    "        rec.append(epsilon) \n",
    "        i = i+1\n",
    "        w0 = w1\n",
    "    mu = w0\n",
    "    v = -delta2(w0,sigma0)\n",
    "    sigma = 1/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真正的w为： \n",
      "[ 0.43213071  4.36282052  2.28311318  0.36502505  1.3315897   1.00102298\n",
      "  4.48223722 -0.61547479  0.9392031  -2.56228722]\n",
      "\n",
      " Laplace的结果为： \n",
      "[ 0.43383022  4.30980619  2.30844433  0.33010292  1.45460567  1.18068878\n",
      "  4.64460481 -0.70787317  0.93355798 -2.67787504]\n"
     ]
    }
   ],
   "source": [
    "print(\"真正的w为： \")\n",
    "print(w)\n",
    "print(\"\\n Laplace的结果为： \")\n",
    "print(mu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
