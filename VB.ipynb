{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayes in Laplace\n",
    "\n",
    "Given the random variable $x$ obey standard normal distribution，$y=\\frac{1}{1+e^{-(Xw^T)}}$, which is the real distribution of $y$\n",
    "\n",
    "## 1.  generation of data\n",
    "\n",
    "### 1.1.  generation of  **$x$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 3. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 3. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 3. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 3. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 3. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 3. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 3.]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "(1000, 10)\n",
      "[[ 1.10139566e+00  3.01397158e+00  5.13868675e-01  1.22543229e+00\n",
      "   3.15720951e+00  7.46113845e-01  2.67208609e+00 -1.56009483e+00\n",
      "  -2.37507485e-01  2.24747278e+00]\n",
      " [ 1.16960397e+00  5.53530823e-02  1.59027534e+00  6.59061729e-01\n",
      "   8.94374723e-01 -6.15292791e-01  3.61612374e-01  5.68824669e-01\n",
      "  -8.62950609e-01 -3.62306365e+00]\n",
      " [-1.43046163e-01  4.25250110e+00 -4.62823029e+00 -1.58184612e+00\n",
      "  -3.93720016e-01  4.66467916e-01  1.95801633e+00  1.80548534e+00\n",
      "   2.25826593e+00  2.40651266e+00]\n",
      " [-1.13700922e+00 -9.74404948e-02 -8.65856736e-01  7.55900541e-01\n",
      "  -6.50927291e-01 -1.59878956e+00  3.32077483e+00 -2.60332159e-01\n",
      "  -1.10631239e+00  1.42854412e+00]\n",
      " [-2.09723422e+00 -8.71923690e-01 -1.21575364e+00 -3.41953772e+00\n",
      "  -4.59986304e+00 -9.99085856e-02 -1.13654851e+00 -1.14610965e+00\n",
      "   1.33255058e+00 -1.55712215e+00]\n",
      " [ 2.93346693e+00 -2.93987087e+00 -4.83825912e+00 -3.91704417e-01\n",
      "   6.88366944e-01  2.87469099e+00 -8.55194568e-01 -6.51420298e-01\n",
      "  -2.93997905e-01  4.18655096e+00]\n",
      " [-3.13300363e+00  5.88466258e-01 -3.93690999e-02 -1.66276372e+00\n",
      "  -6.63573642e-01  1.89711516e-01 -1.48118548e+00  3.83834002e-04\n",
      "   1.14983069e+00  1.29813908e+00]\n",
      " [-8.06821088e-01 -4.80538913e-01  6.14869963e-02  1.46916215e+00\n",
      "   2.82321783e-01  2.09339750e+00  8.70391512e-01 -2.74326036e+00\n",
      "   1.77194598e+00 -1.13105933e+00]\n",
      " [ 9.30189400e-01 -1.38166773e-02  1.60177317e+00  8.07280008e-02\n",
      "   2.45985627e-01 -2.86422552e+00  2.36943183e+00  4.34852931e-02\n",
      "  -6.84995352e-01  2.22462450e+00]]\n"
     ]
    }
   ],
   "source": [
    "########构造x#################\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "np.random.seed(9)####设置随机种子\n",
    "n_samples = 1000####样本量\n",
    "p = 10 ###样本维度\n",
    "var = 3\n",
    "cov = np.identity(p)*var\n",
    "print(cov)####输出生成的协方差矩阵\n",
    "center =np.zeros(10)\n",
    "print(center)###输出生成的均值array\n",
    "global data_x\n",
    "data_x = multivariate_normal.rvs(mean=center, cov=cov, size=1000)\n",
    "print(data_x.shape) \n",
    "print(data_x[1:10])#####输出生成的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. generation of  $w$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(10,)\n",
      "[-2.76599012  3.09612755  3.67479028 -1.24344072  3.44916927  1.91886877\n",
      " -1.663089    0.88682911  1.40901029 -4.17858097]\n",
      "[9.68259013e-01 9.99999979e-01 2.83488275e-05 2.33974438e-09\n",
      " 1.95735896e-02 1.06968208e-19 9.99893300e-01 9.99433964e-01\n",
      " 1.61227251e-07]\n"
     ]
    }
   ],
   "source": [
    "######构造w和y#######\n",
    "global var\n",
    "var = 3\n",
    "#cov = np.identity()*var\n",
    "#print(cov)####输出生成的协方差矩阵\n",
    "center = 0\n",
    "print(center)###输出生成的均值array\n",
    "w = np.random.randn(p)*np.sqrt(var)+center\n",
    "print(w.shape)\n",
    "print(w)\n",
    "global data_y\n",
    "data_y= 1/(1+np.exp(-np.dot(data_x,w)))\n",
    "print(data_y[1:10])\n",
    "y_prob = data_y\n",
    "#maxprob = max(y_prob)\n",
    "global y\n",
    "y = np.zeros(1000)\n",
    "for i in range(y_prob.shape[0]):\n",
    "    if (y_prob[i]>0.5):\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07774498,  0.08103779,  0.09735719, -0.02784708,  0.08229567,\n",
       "        0.06376151, -0.03678785,  0.01552198,  0.02753113, -0.10209337])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.datasets import load_iris\n",
    "from sklearn import linear_model  \n",
    "clf = linear_model.LinearRegression().fit(data_x, y)\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the *coef_* (the $\\hat{w}$) with logistic regression model from sklearn keeps away from the real $w$ , what we need to do is to get the distribution $p(w|X,y)$. \n",
    "According to the Gaussian mixture model(GMM), which can approximate any distribution. Let the $ g(w;\\mu,\\Sigma) \\approx p(w|X,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Kullback–Leibler divergence(K.L.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "KL = E[log\\frac{g(w;\\mu,\\Sigma)}{p(w|X,y)}] \n",
    "= E[log\\frac{g(w;\\mu,\\Sigma)}{\\frac{p(y|X,w)p(w)}{p(y|x)}}]\n",
    "= E[log\\frac{g(w;\\mu,\\Sigma)}{p(y|X,w)p(w)}+logp(y|X)]\n",
    "= E[logg(w;\\mu,\\Sigma) - logp(y|X,w) - logp(w)] +logp(y|X)\n",
    "$$\n",
    "\n",
    "Let the $J=E[logg(w;\\mu,\\Sigma) - logp(y|X,w) - logp(w)]$, what we need to do is to minimize the $J$\n",
    "\n",
    "$$\n",
    "E(logp(y|X,w)) = log[p(y_1|X,w)p(y_2|X,w)p(y_3|X,w)\\cdots p(y_N|X,w)] = \\sum_{n=1}^N log p(y_n|X,w)=\\sum_{n=1}^N log \\frac{1}{1+e^{-X^Tw}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Laplace approximate\n",
    "\n",
    "$$\n",
    "g(w;\\mu,\\Sigma) \\propto log (g(\\hat{w};\\mu,\\Sigma))+ \\frac{\\delta log^{'}(g(w;\\mu,\\Sigma))}{\\delta w}|_{w=\\hat{w}} \\frac{(w-\\hat{w})}{1!}\n",
    "+ \\frac{\\delta log^{''}(g(w;\\mu,\\Sigma))}{\\delta w}|_{w=\\hat{w}} \\frac{(w-\\hat{w})^2}{2!}\n",
    "$$\n",
    "\n",
    "when $w = \\hat{w}$ , the First derivative is 0. $\\hat{w}$ is the value of Newton's method, So:\n",
    "\n",
    "$$\n",
    "g(w;\\mu,\\Sigma) \\propto log (g(\\hat{w};\\mu,\\Sigma))+\\frac{\\delta log^{''}(g(w;\\mu,\\Sigma))}{\\delta w}|_{w=\\hat{w}} \\frac{(w-\\hat{w})^2}{2!}\n",
    "$$\n",
    "\n",
    "Let $K = g(\\hat{w};\\mu,\\Sigma), \\mu = \\hat{w}, \\sigma^2 = \\frac{1}{v}, v =-\\frac{\\delta log^{''}(g(w;\\mu,\\Sigma))}{\\delta w}|_{w=\\hat{w}}$\n",
    "\n",
    "the g is transformed into :\n",
    "$$\n",
    "log (g(w;\\mu,\\Sigma)) \\propto logK - \\frac{(w-\\mu)^2}{2\\sigma^2 }\n",
    "$$\n",
    "As a result, the next step is to solve the $\\hat{w}$\n",
    "\n",
    "### 3.1.  $\\hat{w}$ of solution\n",
    "As we all know , $p(w|x,y) = \\frac{p(y|x,w)p(w)}{p(y|x)}$,then define the $log[g(w;x,y,\\sigma^2)] = log[p(y|x,w)p(w|\\sigma^2)]$. Obviously, the $\\frac{log[g(w;x,y,\\sigma^2)]}{p(w|x,y)} = C$, the $C$ is a constant, so if the maxmum of $log(g)$ is solved, the $\\hat{w}$ is what we need.\n",
    "\n",
    "- **First, we need to transform the function $log(g)$:**\n",
    "\n",
    "$$\n",
    "log[g(w;x,y,\\sigma^2)] = log[p(y|x,w)]+log[p(w|\\sigma^2)]=log[p(y=y_1|x_1,w)\\cdotp(y=y_2|x_2,w)\\cdots p(y=y_n|x_n,w)]+log[p(w|\\sigma^2)]=\\sum_{n=1}^N log[p(y = y_n|x_n,w)]+log[p(w|\\sigma^2)]=\\sum_{n=1}^N log[(\\frac{1}{1+exp(-w^Tx_n)})^{y_n}\\cdot (\\frac{exp(-w^Tx_n)}{1+exp(-w^Tx_n)})^{1-y_n}]+log[p(w|\\sigma^2)]\n",
    "$$\n",
    "\n",
    "- **Let $P_n = P(y_n = 1|w,x_n)$, the formula can be transformed as follows:**\n",
    "\n",
    "$$\n",
    "log[g(w;x,y,\\sigma^2)] =log[p(w|\\sigma^2)] + \\sum_{n=1}^Nlog [P_n^{y_n}\\cdot(1-P_n)^{1-y_n}] = log[p(w|\\sigma^2)] + \\sum_{n=1}^N({y_n}\\cdot log P_n+ (1-y_n)\\cdot log(1-P_n))\n",
    "$$\n",
    "\n",
    "- **$D$ is defined as the dimension of $w$, the formula can be transformed as follows:**\n",
    "$$\n",
    "log[g(w;x,y,\\sigma^2)] = -\\frac{D}{2}log2\\pi-Dlog\\sigma - \\frac{1}{2\\sigma^2}w^Tw+ \\sum_{n=1}^N({y_n}\\cdot log P_n+ (1-y_n)\\cdot log(1-P_n))\n",
    "$$\n",
    "- **the next step is to solve for the first derivative**:\n",
    "$$\n",
    "\\frac{\\delta log[g(w;x,y,\\sigma^2)]}{\\delta w}= - \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N(\\frac{y_n}{P_n}\\frac{\\delta P_n}{\\delta w} - \\frac{1-y_n}{1-P_n}\\frac{\\delta P_n}{\\delta w})=- \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N x_n(y_n-P_n)\n",
    "\\space P_n = \\frac{1}{1+exp(-w^Tx_n)}\n",
    "$$\n",
    "- **the next step is to solve for the second derivative**:\n",
    "$$\n",
    "\\frac{\\delta^2log[g(w;x,y,\\sigma^2)]}{\\delta w^2}= - \\frac{1}{\\sigma^2}-\\sum_{n=1}^N(x_nx_n^TP_n(1-P_n))<0\n",
    "$$\n",
    "Because the second derivative is less than 0, the function must have a maximum. As a result, Newton method is used to solve the zero of the first derivative. Assume that $f(w) = \\frac{\\delta log[g(w;x,y,\\sigma^2)]}{\\delta w}$, $f^{(2)}(w) = \\frac{\\delta^2log[g(w;x,y,\\sigma^2)]}{\\delta w^2}$,\n",
    "$$\n",
    "w_{n+1} = w_n - \\frac{f(w_n)}{f^{(1)}(w_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Gradient Descent\n",
    "$$\n",
    "\\frac{\\delta log[g(w;x,y,\\sigma^2)]}{\\delta w}= - \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N(\\frac{y_n}{P_n}\\frac{\\delta P_n}{\\delta w} - \\frac{1-y_n}{1-P_n}\\frac{\\delta P_n}{\\delta w})=- \\frac{1}{\\sigma^2}w+\\sum_{n=1}^N x_n(y_n-P_n)\n",
    "$$\n",
    "$$\n",
    "w_{n+1} = w_n - \\eta{f^{'}(w_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Question\n",
    "\n",
    "- 如果$\\mu$和$v$都已经可以通过泰勒展开求出来了，那么K.L散度存在的意义是什么，<div style = \"color:red\">KL散度看有多接近</div>\n",
    "- $w$的先验概率是如何确定的.<div style = \"color:red\">先验概率可以先自己随便设置一个</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta1(w): \n",
    "    initial_w = w\n",
    "    derivative1 = -(1/var)*initial_w\n",
    "    temp = np.zeros(data_x.shape)\n",
    "    for i in range(0,data_x.shape[0]):\n",
    "        temp[i,] = data_x[i,]*(y[i]-1/(1+np.exp(-np.dot(data_x[i,],initial_w))))\n",
    "    derivative1 = temp.sum(axis = 0)+derivative1\n",
    "    return (derivative1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta2(w):\n",
    "    initial_w = w\n",
    "    derivative2 = -(1/var)\n",
    "    temp = np.zeros(data_x.shape)\n",
    "    for i in range(0,data_x.shape[0]):\n",
    "        temp[i,] = data_x[i,]*data_x[i,]*(1-1/(1+np.exp(-np.dot(data_x[i,],initial_w))))*1/(1+np.exp(-np.dot(data_x[i,],initial_w)))\n",
    "    derivative2 = temp.sum(axis = 0)-derivative2\n",
    "    return(derivative2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal(w):\n",
    "    D = w.shape[0]\n",
    "    tempsum = 0\n",
    "    for i in range(0,data_x.shape[0]):\n",
    "        p = 1/(1+np.exp(-np.dot(data_x[i,],w)))\n",
    "        tempsum = tempsum + y[i] * np.log(p) + (1-y[i]) * (1-np.log(p))\n",
    "    res = tempsum- (D/2)*np.log(2*np.pi) - D * np.log(var) - 1/(2*(var**2))*np.dot(w.T,w)\n",
    "    #print(tempsum-res)\n",
    "    return (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "log[g(w;x,y,\\sigma^2)] = -\\frac{D}{2}log2\\pi-Dlog\\sigma - \\frac{1}{2\\sigma^2}w^Tw+ \\sum_{n=1}^N({y_n}\\cdot log P_n+ (1-y_n)\\cdot log(1-P_n))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 牛顿法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_w = np.random.randn(p)*np.sqrt(var)####利用w的先验随机生成w\n",
    "wn = initial_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    wn1 = wn - delta1(wn)/delta2(wn)\n",
    "    #print( delta1(wn))\n",
    "    #print(delta2(wn))\n",
    "    delta = wn1 - wn \n",
    "    print(delta)\n",
    "    print(wn1)\n",
    "    if(max(abs(delta))<0.0001):\n",
    "        break\n",
    "    wn = wn1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python 优化函数。|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. 梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.37962924,  3.87379924,  4.43482696, -1.54002588,  4.30916493,\n",
       "        2.45393028, -1.98541913,  1.10206495,  1.66212422, -5.15182932])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4180.899244495515\n",
      "527.8167318090555\n",
      "10.293798610252452\n",
      "27.34635808153871\n",
      "30.369675082154572\n",
      "30.879081716758265\n",
      "30.219035236749733\n",
      "29.616232562242658\n",
      "28.997125338755723\n",
      "28.41000815287589\n",
      "27.841451603077076\n",
      "27.2925890893639\n",
      "26.761359145792085\n",
      "26.246947459463627\n",
      "25.74844931583084\n",
      "25.265103725691915\n",
      "24.796196173384487\n",
      "24.34106438949675\n",
      "23.899088165298963\n",
      "23.469686107155212\n",
      "23.05231215927961\n",
      "22.64645285312963\n",
      "22.251624816855838\n",
      "21.867372540422366\n",
      "21.493266343724827\n",
      "21.128900524659002\n",
      "20.773891667497082\n",
      "20.427877094294672\n",
      "20.09051344620093\n",
      "19.761475380812954\n",
      "19.440454375540867\n",
      "19.127157626176086\n",
      "18.821307032413642\n",
      "18.52263826208491\n",
      "18.230899886890256\n",
      "17.945852583614396\n",
      "17.6672683947354\n",
      "17.394930043439672\n",
      "17.12863029838354\n",
      "16.868171383907793\n",
      "16.61336443199616\n",
      "16.36402897251537\n",
      "16.119992458717206\n",
      "15.881089824731134\n",
      "15.647163073365846\n",
      "15.418060890861852\n",
      "15.193638287165413\n",
      "14.973756259685615\n",
      "14.758281478249955\n",
      "14.547085990544474\n",
      "14.34004694567193\n",
      "14.137046335039486\n",
      "13.937970749116175\n",
      "13.742711148813214\n",
      "13.551162650964216\n",
      "13.363224326047202\n",
      "13.178799008182978\n",
      "12.997793116050161\n",
      "12.820116484060236\n",
      "12.645682203183242\n",
      "12.474406470872964\n",
      "12.306208449047517\n",
      "12.141010130333598\n",
      "11.97873621130475\n",
      "11.819313972732743\n",
      "11.662673166200875\n",
      "11.508745906819058\n",
      "11.357466571518671\n",
      "11.208771702662489\n",
      "11.062599916695945\n",
      "10.918891817276744\n",
      "10.777589913141128\n",
      "10.638638539796375\n",
      "10.501983785189623\n",
      "10.367573419209293\n",
      "10.235356826308816\n",
      "10.105284941838363\n",
      "9.977310190951357\n",
      "9.851386430749699\n",
      "9.72746889516111\n",
      "9.605514142141146\n",
      "9.48548000360097\n",
      "9.367325537546094\n",
      "9.251010982274238\n",
      "9.136497712857818\n",
      "9.02374819947272\n",
      "8.91272596757608\n",
      "8.803395559876662\n",
      "8.69572249988596\n",
      "8.589673257277354\n",
      "8.485215214442178\n",
      "8.382316634735616\n",
      "8.280946631867664\n",
      "8.18107514071653\n",
      "8.082672889461719\n",
      "7.985711372416517\n",
      "7.890162824734944\n",
      "7.7960001972314785\n",
      "7.703197133204412\n",
      "7.611727945288294\n",
      "7.521567593952568\n",
      "7.432691666332175\n",
      "7.345076356226855\n",
      "7.258698444788024\n",
      "7.173535281764089\n",
      "7.089564767835327\n",
      "7.006765337318939\n",
      "6.925115941709009\n",
      "6.844596033684866\n",
      "6.7651855518688535\n",
      "6.686864906092524\n",
      "6.609614963203057\n",
      "6.533417033332626\n",
      "6.458252856866238\n",
      "6.3841045915633\n",
      "6.310954800510444\n",
      "6.238786440196236\n",
      "6.1675828492134315\n",
      "6.097327737220439\n",
      "6.028005174428472\n",
      "5.959599581283328\n",
      "5.892095718712881\n",
      "5.825478678486434\n",
      "5.7597338741379644\n",
      "5.694847031996687\n",
      "5.630804182562315\n",
      "5.567591652390547\n",
      "5.5051960557930215\n",
      "5.443604287355811\n",
      "5.382803514315128\n",
      "5.322781169224072\n",
      "5.263524943125958\n",
      "5.205022778557577\n",
      "5.147262863218202\n",
      "5.090233623366657\n",
      "5.033923717757716\n",
      "4.9783220317704036\n",
      "4.923417671512652\n",
      "4.869199958228819\n",
      "4.815658423021887\n",
      "4.762782801316462\n",
      "4.710563028072102\n",
      "4.658989232691056\n",
      "4.608051734247965\n",
      "4.557741036757761\n",
      "4.508047824933783\n",
      "4.458962959514793\n",
      "4.410477473211358\n",
      "4.362582566524907\n",
      "4.315269603787783\n",
      "4.268530109264248\n",
      "4.2223557633697055\n",
      "4.176738399056376\n",
      "4.131669998208054\n",
      "4.087142688302265\n",
      "4.043148738869604\n",
      "3.9996805584742106\n",
      "3.956730691273151\n",
      "3.9142918142861163\n",
      "3.8723567340002774\n",
      "3.830918383831886\n",
      "3.7899698210640054\n",
      "3.7495042242071577\n",
      "3.709514890230821\n",
      "3.6699952320896045\n",
      "3.630938776097537\n",
      "3.5923391594205896\n",
      "3.5541901278484147\n",
      "3.5164855332777734\n",
      "3.479219331546119\n",
      "3.442385580194241\n",
      "3.405978436253463\n",
      "3.3699921542665834\n",
      "3.3344210841578388\n",
      "3.299259669170169\n",
      "3.2645024441344503\n",
      "3.2301440333540086\n",
      "3.1961791489493407\n",
      "3.162602588896334\n",
      "3.129409235388266\n",
      "3.0965940530559237\n",
      "3.0641520874078196\n",
      "3.0320784630803246\n",
      "3.0003683822606035\n",
      "2.96901712332874\n",
      "2.93802003908786\n",
      "2.9073725554590055\n",
      "2.8770701700395875\n",
      "2.8471084506090847\n",
      "2.8174830338984975\n",
      "2.788189624068764\n",
      "2.7592239916275503\n",
      "2.730581972021355\n",
      "2.702259464393137\n",
      "2.674252430406341\n",
      "2.6465568931162124\n",
      "2.6191689356683128\n",
      "2.5920847003380914\n",
      "2.5653003872457703\n",
      "2.5388122534359354\n",
      "2.5126166118288893\n",
      "2.4867098299664576\n",
      "2.4610883292934886\n",
      "2.4357485839873334\n",
      "2.410687120081093\n",
      "2.3859005145286574\n",
      "2.3613853941305933\n",
      "2.3371384349002255\n",
      "2.3131563609995283\n",
      "2.289435943874196\n",
      "2.2659740014978524\n",
      "2.242767397555326\n",
      "2.2198130405213305\n",
      "2.197107882990167\n",
      "2.1746489208690036\n",
      "2.15243319261117\n",
      "2.130457778467644\n",
      "2.1087197998176634\n",
      "2.0872164183901987\n",
      "2.065944835640039\n",
      "2.0449022920374773\n",
      "2.024086066398013\n",
      "2.0034934751956825\n",
      "1.9831218721064943\n",
      "1.9629686471107561\n",
      "1.9430312260865321\n",
      "1.9233070701484394\n",
      "1.9037936751074085\n",
      "1.8844885707303547\n",
      "1.8653893204318592\n",
      "1.8464935205283837\n",
      "1.827798799809898\n",
      "1.8093028188823155\n",
      "1.7910032698055147\n",
      "1.7728978755085336\n",
      "1.754984389252968\n",
      "1.7372605942155133\n",
      "1.7197243029977471\n",
      "1.7023733570622426\n",
      "1.685205626466086\n",
      "1.6682190092333258\n",
      "1.6514114309184151\n",
      "1.634780844317902\n",
      "1.6183252289420125\n",
      "1.6020425905117008\n",
      "1.5859309608194962\n",
      "1.569988396951885\n",
      "1.5542129812611165\n",
      "1.5386028207467461\n",
      "1.523156046711847\n",
      "1.5078708144947086\n",
      "1.4927453029913522\n",
      "1.4777777143071944\n",
      "1.4629662735342208\n",
      "1.4483092281616337\n",
      "1.4338048479830832\n",
      "1.4194514246501058\n",
      "1.405247271335611\n",
      "1.3911907224101014\n",
      "1.3772801331688242\n",
      "1.363513879522543\n",
      "1.3498903576210068\n",
      "1.3364079836856035\n",
      "1.3230651935355127\n",
      "1.3098604424840232\n",
      "1.2967922049128902\n",
      "1.2838589740558746\n",
      "1.271059261793198\n",
      "1.2583915981986138\n",
      "1.2458545315075753\n",
      "1.2334466276215608\n",
      "1.221166470026219\n",
      "1.209012659518521\n",
      "1.1969838138838895\n",
      "1.1850785676697342\n",
      "1.1732955720754035\n",
      "1.1616334944892515\n",
      "1.1500910184768145\n",
      "1.1386668434706735\n",
      "1.1273596844348504\n",
      "1.1161682719266537\n",
      "1.1050913515819047\n",
      "1.0941276840712817\n",
      "1.0832760448301997\n",
      "1.0725352239269341\n",
      "1.0619040257797678\n",
      "1.0513812690123814\n",
      "1.0409657862246604\n",
      "1.0306564237944258\n",
      "1.0204520417819367\n",
      "1.0103515135933776\n",
      "1.0003537259763107\n",
      "0.9904575786313217\n",
      "0.9806619842365762\n",
      "0.9709658681376823\n",
      "0.9613681683122195\n",
      "0.9518678349686525\n",
      "0.9424638306800261\n",
      "0.933155129961051\n",
      "0.9239407193163061\n",
      "0.9148195968846267\n",
      "0.9057907724400138\n",
      "0.8968532672206493\n",
      "0.8880061136260338\n",
      "0.879248355296113\n",
      "0.8705790468202395\n",
      "0.8619972535952911\n",
      "0.8535020517547309\n",
      "0.8450925279867079\n",
      "0.8367677793676194\n",
      "0.8285269133166366\n",
      "0.8203690473865208\n",
      "0.8122933091117375\n",
      "0.8042988360211893\n",
      "0.796384775295337\n",
      "0.7885502838753382\n",
      "0.7807945281238062\n",
      "0.7731166838620993\n",
      "0.765515936211159\n",
      "0.7579914793905118\n",
      "0.7505425166946225\n",
      "0.7431682604064918\n",
      "0.735867931555731\n",
      "0.7286407599131053\n",
      "0.7214859838577468\n",
      "0.7144028502452784\n",
      "0.707390614353244\n",
      "0.7004485397228564\n",
      "0.693575898074414\n",
      "0.6867719691526872\n",
      "0.6800360408506094\n",
      "0.6733674087290638\n",
      "0.6667653763006456\n",
      "0.6602292546685931\n",
      "0.6537583625995467\n",
      "0.6473520262761667\n",
      "0.6410095793899018\n",
      "0.6347303628745067\n",
      "0.6285137249506079\n",
      "0.622359020917429\n",
      "0.616265613169162\n",
      "0.6102328711167502\n",
      "0.6042601709605151\n",
      "0.5983468957538207\n",
      "0.5924924352766539\n",
      "0.5866961859919684\n",
      "0.5809575507355476\n",
      "0.5752759391261861\n",
      "0.569650766932682\n",
      "0.564081456378517\n",
      "0.5585674358635515\n",
      "0.5531081400622497\n",
      "0.5477030096753879\n",
      "0.5423514914655243\n",
      "0.5370530381560457\n",
      "0.5318071083593168\n",
      "0.5266131665075591\n",
      "0.5214706828437556\n",
      "0.5163791331233369\n",
      "0.5113379989852547\n",
      "0.5063467673826381\n",
      "0.5014049308911126\n",
      "0.496511987453232\n",
      "0.49166744042213395\n",
      "0.4868707983832792\n",
      "0.4821215752399439\n",
      "0.4774192900340495\n",
      "0.4727634668679457\n",
      "0.4681536349835369\n",
      "0.46358932863586233\n",
      "0.4590700869248394\n",
      "0.4545954538925798\n",
      "0.4501649784797337\n",
      "0.44577821425627917\n",
      "0.44143471959159797\n",
      "0.4371340575708018\n",
      "0.43287579574189294\n",
      "0.4286595064204448\n",
      "0.42448476623849274\n",
      "0.4203511563609936\n",
      "0.4162582623812341\n",
      "0.4122056742080531\n",
      "0.4081929860976743\n",
      "0.4042197965309242\n",
      "0.4002857081732145\n",
      "0.39639032793274964\n",
      "0.392533266760438\n",
      "0.3887141397017331\n",
      "0.3849325658629823\n",
      "0.38118816824771784\n",
      "0.3774805738548821\n",
      "0.3738094135560459\n",
      "0.3701743220944991\n",
      "0.3665749379360932\n",
      "0.3630109034156703\n",
      "0.35948186452696973\n",
      "0.3559874709244468\n",
      "0.35252737597147643\n",
      "0.3491012365302595\n",
      "0.3457087131200751\n",
      "0.34234946969718294\n",
      "0.3390231737657814\n",
      "0.33572949619156134\n",
      "0.3324681113317638\n",
      "0.3292386968369101\n",
      "0.3260409337217425\n",
      "0.3228745062751841\n",
      "0.31973910206306755\n",
      "0.3166344118744746\n",
      "0.3135601296380628\n",
      "0.31051595244753116\n",
      "0.3075015805934527\n",
      "0.30451671734135743\n",
      "0.30156106899721635\n",
      "0.29863434503840836\n",
      "0.2957362577362801\n",
      "0.292866522404438\n",
      "0.29002485728506144\n",
      "0.2872109834834191\n",
      "0.28442462491238985\n",
      "0.2816655084197919\n",
      "0.2789333635528237\n",
      "0.27622792264901364\n",
      "0.2735489207698265\n",
      "0.2708960957661475\n",
      "0.26826918801725697\n",
      "0.26566794064910937\n",
      "0.2630920993951804\n",
      "0.2605414125482639\n",
      "0.25801563099685154\n",
      "0.2555145081396404\n",
      "0.2530377998846234\n",
      "0.25058526463271846\n",
      "0.2481566632486647\n",
      "0.24575175899917667\n",
      "0.24337031755294447\n",
      "0.24101210700246156\n",
      "0.23867689771486766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2363644624401786\n",
      "0.23407457626126416\n",
      "0.2318070164465098\n",
      "0.22956156258078408\n",
      "0.22733799644083774\n",
      "0.2251361020726108\n",
      "0.22295566561388114\n",
      "0.2207964754779823\n",
      "0.21865832207913627\n",
      "0.21654099809074978\n",
      "0.21444429812618182\n",
      "0.21236801898157864\n",
      "0.21031195950581605\n",
      "0.2082759204440663\n",
      "0.20625970471701294\n",
      "0.20426311711798917\n",
      "0.202285964438488\n",
      "0.20032805541450216\n",
      "0.19838920070560562\n",
      "0.1964692128576644\n",
      "0.19456790630647447\n",
      "0.19268509737594286\n",
      "0.19082060419532354\n",
      "0.1889742467619726\n",
      "0.18714584681492852\n",
      "0.18533522794677992\n",
      "0.18354221547633642\n",
      "0.1817666364941033\n",
      "0.18000831978406495\n",
      "0.17826709591281542\n",
      "0.17654279709040566\n",
      "0.17483525720308535\n",
      "0.17314431180420797\n",
      "0.1714697981315112\n",
      "0.16981155497978762\n",
      "0.16816942280911462\n",
      "0.1665432436593619\n",
      "0.16493286112108763\n",
      "0.16333812038283213\n",
      "0.1617588681201596\n",
      "0.16019495265300066\n",
      "0.15864622365825198\n",
      "0.15711253241533996\n",
      "0.15559373167798185\n",
      "0.1540896756332586\n",
      "0.1526002199107097\n",
      "0.15112522162962705\n",
      "0.14966453928172996\n",
      "0.1482180328175673\n",
      "0.1467855634946318\n",
      "0.1453669940265172\n",
      "0.14396218849287834\n",
      "0.14257101223211066\n",
      "0.14119333201779227\n",
      "0.13982901593863062\n",
      "0.13847793333570735\n",
      "0.13713995487614739\n",
      "0.13581495252037712\n",
      "0.13450279948210664\n",
      "0.1332033702356057\n",
      "0.13191654048841883\n",
      "0.13064218724593957\n",
      "0.12938018859858857\n",
      "0.1281304239828387\n",
      "0.12689277393110387\n",
      "0.1256671202572761\n",
      "0.12445334580115741\n",
      "0.1232513347058557\n",
      "0.12206097215312184\n",
      "0.12088214453706314\n",
      "0.11971473932953813\n",
      "0.11855864513199776\n",
      "0.11741375161727774\n",
      "0.1162799496196385\n",
      "0.11515713096196123\n",
      "0.1140451885830771\n",
      "0.11294401647683117\n",
      "0.1118535096748019\n",
      "0.11077356425630569\n",
      "0.10970407732929743\n",
      "0.108644946950335\n",
      "0.10759607228010282\n",
      "0.10655735339696548\n",
      "0.10552869143430144\n",
      "0.10450998840042303\n",
      "0.10350114737866534\n",
      "0.10250207235003472\n",
      "0.1015126681959373\n",
      "0.10053284084642655\n",
      "0.09956249702554487\n",
      "0.09860154451234848\n",
      "0.0976498918953439\n",
      "0.0967074486788988\n",
      "0.09577412529415597\n",
      "0.09484983299626037\n",
      "0.09393448397077009\n",
      "0.09302799124270678\n",
      "0.09213026863199048\n",
      "0.09124123094534298\n",
      "0.09036079367888306\n",
      "0.08948887324822863\n",
      "0.08862538687480992\n",
      "0.08777025253766624\n",
      "0.08692338910987019\n",
      "0.08608471616571478\n",
      "0.08525415417716431\n",
      "0.08443162428011419\n",
      "0.08361704845447093\n",
      "0.08281034945048305\n",
      "0.0820114507632752\n",
      "0.0812202765564507\n",
      "0.0804367519031075\n",
      "0.07966080244750628\n",
      "0.07889235467791877\n",
      "0.07813133573927189\n",
      "0.07737767348044144\n",
      "0.07663129654338263\n",
      "0.07589213414212281\n",
      "0.07516011630195862\n",
      "0.07443517365845764\n",
      "0.07371723754658888\n",
      "0.0730062399761664\n",
      "0.07230211363912531\n",
      "0.0716047918303957\n",
      "0.07091420856977493\n",
      "0.07023029845368\n",
      "0.06955299681612814\n",
      "0.0688822394622548\n",
      "0.06821796298117988\n",
      "0.06756010453955241\n",
      "0.0669086018251619\n",
      "0.06626339326612651\n",
      "0.0656244178153429\n",
      "0.0649916150541685\n",
      "0.06436492512966652\n",
      "0.06374428875915328\n",
      "0.06312964731569082\n",
      "0.06252094266255881\n",
      "0.061918117233290104\n",
      "0.06132111411807273\n",
      "0.06072987683364772\n",
      "0.06014434951430303\n",
      "0.059564476859122806\n",
      "0.05899020409651712\n",
      "0.058421476908733894\n",
      "0.0578582416055724\n",
      "0.05730044502161036\n",
      "0.05674803441797849\n",
      "0.056200957666078466\n",
      "0.05565916309024033\n",
      "0.055122599514106696\n",
      "0.05459121633157338\n",
      "0.054064963303972036\n",
      "0.05354379083655658\n",
      "0.05302764968837437\n",
      "0.05251649114507018\n",
      "0.052010267001605825\n",
      "0.05150892946221575\n",
      "0.05101243118770071\n",
      "0.050520725412752654\n",
      "0.05003376571312401\n",
      "0.049551506141142454\n",
      "0.049073901203883\n",
      "0.048600905879538914\n",
      "0.04813247556103306\n",
      "0.04766856604055647\n",
      "0.047209133607793774\n",
      "0.04675413492350344\n",
      "0.04630352711137675\n",
      "0.04585726766163134\n",
      "0.04541531451741321\n",
      "0.04497762605842581\n",
      "0.04454416094904445\n",
      "0.0441148784248071\n",
      "0.04368973796226783\n",
      "0.04326869954547874\n",
      "0.04285172346953914\n",
      "0.04243877048702416\n",
      "0.04202980162244785\n",
      "0.04162477841509826\n",
      "0.041223662695301755\n",
      "0.04082641665627307\n",
      "0.04043300290231855\n",
      "0.040043384384262026\n",
      "0.039657524412177736\n",
      "0.03927538661082508\n",
      "0.038896935054253845\n",
      "0.03852213405116345\n",
      "0.03815094835499622\n",
      "0.037783343002956826\n",
      "0.03741928340605227\n",
      "0.037058735263599374\n",
      "0.03670166467054514\n",
      "0.03634803796921915\n",
      "0.03599782193759893\n",
      "0.035650983574669226\n",
      "0.03530749024866964\n",
      "0.03496730964889139\n",
      "0.03463040974838805\n",
      "0.03429675886854966\n",
      "0.03396632557723933\n",
      "0.033639078849773796\n",
      "0.03331498784245923\n",
      "0.032994022117236455\n",
      "0.03267615141794522\n",
      "0.03236134588769346\n",
      "0.032049575942437514\n",
      "0.03174081219822256\n",
      "0.03143502568036638\n",
      "0.031132187566981884\n",
      "0.030832269424536207\n",
      "0.030535243045051175\n",
      "0.030241080517043883\n",
      "0.02994975411820633\n",
      "0.02966123654096009\n",
      "0.02937550059050409\n",
      "0.029092519444930076\n",
      "0.0288122664887851\n",
      "0.028534715396745014\n",
      "0.02825984007176885\n",
      "0.027987614644189307\n",
      "0.027718013587218593\n",
      "0.027451011483208276\n",
      "0.02718658334288193\n",
      "0.026924704236080288\n",
      "0.026665349582799536\n",
      "0.026408495007672173\n",
      "0.026154116380894266\n",
      "0.025902189784574148\n",
      "0.025652691599134414\n",
      "0.02540559829594713\n",
      "0.02516088675838546\n",
      "0.024918533911659324\n",
      "0.024678517072970863\n",
      "0.02444081360954442\n",
      "0.024205401257859194\n",
      "0.023972257879904646\n",
      "0.023741361585962295\n",
      "0.023512690696406935\n",
      "0.0232862236962319\n",
      "0.023061939385115693\n",
      "0.02283981661184953\n",
      "0.022619834605393407\n",
      "0.022401972668376402\n",
      "0.02218621031352086\n",
      "0.02197252733822097\n",
      "0.02176090366265271\n",
      "0.02155131939343846\n",
      "0.021343754867302778\n",
      "0.02113819058831723\n",
      "0.020934607305207464\n",
      "0.02073298583036376\n",
      "0.020533307291771052\n",
      "0.020335552947472024\n",
      "0.020139704185567098\n",
      "0.019945742665186117\n",
      "0.019753650162783742\n",
      "0.019563408677640837\n",
      "0.019375000317268132\n",
      "0.01918840739654115\n",
      "0.01900361247044202\n",
      "0.01882059808485792\n",
      "0.018639347168573295\n",
      "0.018459842654010572\n",
      "0.018282067719155748\n",
      "0.018106005684785487\n",
      "0.017931640009919647\n",
      "0.01775895435912389\n",
      "0.017587932500646275\n",
      "0.01741855842556106\n",
      "0.017250816215891973\n",
      "0.017084690137380676\n",
      "0.016920164588555053\n",
      "0.016757224178036267\n",
      "0.016595853608123434\n",
      "0.016436037684798066\n",
      "0.016277761497804022\n",
      "0.01612101014416112\n",
      "0.015965768957357795\n",
      "0.015812023316357227\n",
      "0.015659758890251396\n",
      "0.015508961307205027\n",
      "0.015359616495516093\n",
      "0.015211710383482568\n",
      "0.015065229147694481\n",
      "0.014920159056600824\n",
      "0.014776486447772186\n",
      "0.014634197904342727\n",
      "0.014493280049464374\n",
      "0.014353719695463951\n",
      "0.01421550372378988\n",
      "0.014078619219617394\n",
      "0.01394305326630274\n",
      "0.013808793245516426\n",
      "0.013675826517101086\n",
      "0.013544140599151433\n",
      "0.013413723201665562\n",
      "0.013284562049193482\n",
      "0.013156645047274651\n",
      "0.01302996017238911\n",
      "0.012904495617476641\n",
      "0.01278023954910168\n",
      "0.012657180366659304\n",
      "0.012535306523204781\n",
      "0.0124146065536479\n",
      "0.012295069203901221\n",
      "0.012176683208963368\n",
      "0.01205943753302563\n",
      "0.011943321146645758\n",
      "0.01182832313770632\n",
      "0.011714432810549624\n",
      "0.011601639406762843\n",
      "0.011489932411677728\n",
      "0.011379301323358959\n",
      "0.011269735763562494\n",
      "0.011161225492287485\n",
      "0.011053760337745189\n",
      "0.010947330226372287\n",
      "0.010841925130080199\n",
      "0.010737535259067954\n",
      "0.010634150761688943\n",
      "0.010531761976380949\n",
      "0.010430359338897688\n",
      "0.010329933257708035\n",
      "0.010230474405943824\n",
      "0.010131973425814067\n",
      "0.010034421111413394\n",
      "0.009937808280483296\n",
      "0.009842125932664203\n",
      "0.00974736504667817\n",
      "0.009653516779508209\n",
      "0.00956057234088803\n",
      "0.009468522986026073\n",
      "0.009377360142025282\n",
      "0.009287075215070217\n",
      "0.009197659753226617\n",
      "0.009109105440984422\n",
      "0.009021403899168945\n",
      "0.008934546985074121\n",
      "0.008848526477777341\n",
      "0.008763334382820176\n",
      "0.008678962711201166\n",
      "0.00859540350666066\n",
      "0.00851264904213167\n",
      "0.00843069146048947\n",
      "0.008349523128345027\n",
      "0.008269136455965054\n",
      "0.008189523900000495\n",
      "0.008110677982585912\n",
      "0.008032591350456642\n",
      "0.00795525666217145\n",
      "0.007878666683609481\n",
      "0.007802814245223999\n",
      "0.0077276922447708785\n",
      "0.007653293610928813\n",
      "0.007579611436995037\n",
      "0.007506638780796493\n",
      "0.007434368807480496\n",
      "0.0073627947367640445\n",
      "0.007291909896594007\n",
      "0.007221707615826745\n",
      "0.007152181367928279\n",
      "0.007083324583618378\n",
      "0.0070151308391359635\n",
      "0.00694759374619025\n",
      "0.006880706981064577\n",
      "0.006814464299168321\n",
      "0.00674885945772985\n",
      "0.006683886316750431\n",
      "0.006619538823542825\n",
      "0.00655581093087676\n",
      "0.006492696654277097\n",
      "0.0064301901029466535\n",
      "0.006368285467942769\n",
      "0.006306976842097356\n",
      "0.006246258587452758\n",
      "0.006186124947817007\n",
      "0.006126570333435666\n",
      "0.006067589181839139\n",
      "0.006009175904182484\n",
      "0.005951325085334247\n",
      "0.0058940312892445945\n",
      "0.005837289161718218\n",
      "0.005781093361292733\n",
      "0.005725438650188153\n",
      "0.0056703198424656875\n",
      "0.005615731693069392\n",
      "0.005561669195230934\n",
      "0.005508127210305247\n",
      "0.005455100746075914\n",
      "0.005402584847615799\n",
      "0.0053505746018345235\n",
      "0.005299065112922108\n",
      "0.005248051576927537\n",
      "0.005197529210818175\n",
      "0.005147493271579151\n",
      "0.005097939144434349\n",
      "0.005048862076364458\n",
      "0.00500025754354283\n",
      "0.004952121016685851\n",
      "0.004904447901026288\n",
      "0.0048572338182566455\n",
      "0.004810474320038338\n",
      "0.004764165010783472\n",
      "0.004718301554021309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004672879714235023\n",
      "0.004627895161320339\n",
      "0.00458334372342506\n",
      "0.004539221229606483\n",
      "0.004495523529840284\n",
      "0.004452246560504136\n",
      "0.0044093862425143016\n",
      "0.004366938598650449\n",
      "0.00432489960348903\n",
      "0.004283265380763623\n",
      "0.0042420319659868255\n",
      "0.00420119556656573\n",
      "0.004160752329880779\n",
      "0.004120698438782711\n",
      "0.004081030207998992\n",
      "0.00404174387040257\n",
      "0.0040028357598203\n",
      "0.003964302260101249\n",
      "0.003926139741452062\n",
      "0.003888344623192097\n",
      "0.0038509133864863543\n",
      "0.0038138425297802314\n",
      "0.003777128575165989\n",
      "0.003740768056559318\n",
      "0.003704757607920328\n",
      "0.0036690938813990215\n",
      "0.003633773465480772\n",
      "0.003598793118726462\n",
      "0.0035641495369418408\n",
      "0.0035298394668643596\n",
      "0.0034958597152581206\n",
      "0.0034622071052581305\n",
      "0.003428878490922216\n",
      "0.0033958707263082033\n",
      "0.003363180740052485\n",
      "0.0033308054798908415\n",
      "0.0032987418926495593\n",
      "0.0032669869942765217\n",
      "0.0032355378007196123\n",
      "0.003204391398867301\n",
      "0.003173544807395956\n",
      "0.0031429952223334112\n",
      "0.0031127397223826847\n",
      "0.003082775500843127\n",
      "0.003053099747376109\n",
      "0.003023709688022791\n",
      "0.002994602555190795\n",
      "0.0029657756567758042\n",
      "0.0029372262870310806\n",
      "0.002908951743847865\n",
      "0.002880949410609901\n",
      "0.002853216652511037\n",
      "0.002825750868396426\n",
      "0.002798549503495451\n",
      "0.0027716100194083992\n",
      "0.0027449298549981904\n",
      "0.002718506566452561\n",
      "0.0026923376290142187\n",
      "0.0026664206434361404\n",
      "0.002640753144987684\n",
      "0.002615332731693343\n",
      "0.002590157049780828\n",
      "0.0025652237427493674\n",
      "0.0025405304513697047\n",
      "0.002516074881896202\n",
      "0.0024918547287597903\n",
      "0.002467867755512998\n",
      "0.0024441117075184593\n",
      "0.0024205843392337556\n",
      "0.002397283454229182\n",
      "0.002374206898821285\n",
      "0.002351352483856317\n",
      "0.002328718106582528\n",
      "0.0023063015960360644\n",
      "0.0022841008849354694\n",
      "0.0022621139096372644\n",
      "0.002240338607407466\n",
      "0.0022187728764038184\n",
      "0.002197414809415932\n",
      "0.0021762623200629605\n",
      "0.002155313479306642\n",
      "0.0021345662680687383\n",
      "0.002114018805514206\n",
      "0.0020936691553288256\n",
      "0.0020735153930218075\n",
      "0.002053555639577098\n",
      "0.002033788027802075\n",
      "0.0020142107214269345\n",
      "0.001994821870539454\n",
      "0.0019756196679736604\n",
      "0.0019566023192965076\n",
      "0.001937768030074949\n",
      "0.0019191150795450085\n",
      "0.0019006416569027351\n",
      "0.0018823460768544464\n",
      "0.001864226629550103\n",
      "0.0018462816051396658\n",
      "0.0018285093256054097\n",
      "0.0018109081402144511\n",
      "0.0017934763827724964\n",
      "0.0017762124280125136\n",
      "0.0017591146615814068\n",
      "0.001742181504596374\n",
      "0.0017254113217859413\n",
      "0.0017088026315832394\n",
      "0.0016923537423281232\n",
      "0.0016760632543082465\n",
      "0.0016599295868218178\n",
      "0.001643951196456328\n",
      "0.0016281266453006538\n",
      "0.001612454414498643\n",
      "0.0015969330497682677\n",
      "0.0015815611050129519\n",
      "0.0015663371350456146\n",
      "0.0015512597019551322\n",
      "0.0015363274487754097\n",
      "0.001521538878478168\n",
      "0.00150689271686133\n",
      "0.0014923875205568038\n",
      "0.0014780219580643461\n",
      "0.0014637946851507877\n",
      "0.001449704373953864\n",
      "0.0014357496984302998\n",
      "0.0014219293407222722\n",
      "0.0014082420293561881\n",
      "0.00139468647648755\n",
      "0.0013812614133712486\n",
      "0.0013679655912710587\n",
      "0.0013547977378038922\n",
      "0.001341756652436743\n",
      "0.0013288411082612583\n",
      "0.0013160498838260537\n",
      "0.0013033817831455963\n",
      "0.0012908356457046466\n",
      "0.0012784102664227248\n",
      "0.0012661045120694325\n",
      "0.0012539171857497422\n",
      "0.0012418471960700117\n",
      "0.0012298934061618638\n",
      "0.0012180546664239955\n",
      "0.0012063298672728706\n",
      "0.0011947179973503808\n",
      "0.0011832178524855408\n",
      "0.0011718284204107476\n",
      "0.0011605486142798327\n",
      "0.001149377409092267\n",
      "0.0011383137543816702\n",
      "0.0011273565542069264\n",
      "0.0011165048399561783\n",
      "0.0011057576120947488\n",
      "0.0010951138137897942\n",
      "0.001084572501895309\n",
      "0.001074132630492386\n",
      "0.001063793263710977\n",
      "0.0010535534138398361\n",
      "0.0010434121650177985\n",
      "0.0010333685067962506\n",
      "0.0010234215324089746\n",
      "0.0010135703232663218\n",
      "0.0010038139334938023\n",
      "0.0009941514690581243\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01####学习率\n",
    "#var = 5\n",
    "initial_w = np.random.randn(p)*np.sqrt(var)####利用w的先验随机生成w\n",
    "w0 = initial_w\n",
    "epsilon = 1\n",
    "i = 0\n",
    "rec = []\n",
    "while(epsilon>0.001):\n",
    "    w1 = w0 + eta * delta1(w0)\n",
    "    epsilon = abs(cal(w1) - cal(w0))\n",
    "    rec.append(epsilon) \n",
    "    i = i+1\n",
    "    w0 = w1\n",
    "    print(epsilon)\n",
    "mu = w0\n",
    "v = -delta2(w0)\n",
    "sigma = 1/v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 生成大量的$w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.76599012,  3.09612755,  3.67479028, -1.24344072,  3.44916927,\n",
       "        1.91886877, -1.663089  ,  0.88682911,  1.40901029, -4.17858097])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.37962924,  3.87379924,  4.43482696, -1.54002588,  4.30916493,\n",
       "        2.45393028, -1.98541913,  1.10206495,  1.66212422, -5.15182932])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01483976, -0.02105108, -0.02079754, -0.01687106, -0.01979979,\n",
       "       -0.01581618, -0.01593338, -0.01546129, -0.01360087, -0.020146  ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n"
     ]
    }
   ],
   "source": [
    "w_hat = np.matrix(np.zeros(w.shape[0]*100).reshape(w.shape[0],100))\n",
    "for j in range(0,w.shape[0]):\n",
    "    w_hat[j,] = np.random.randn(100)*sigma[j]+mu[j]\n",
    "print(w_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-3.37797188, -3.39639901, -3.3748149 , -3.37046056, -3.40652584,\n",
       "         -3.39589622, -3.3823633 , -3.40532868, -3.40162513, -3.38182774,\n",
       "         -3.38137078, -3.39132401, -3.38775465, -3.3828818 , -3.41670593,\n",
       "         -3.35098014, -3.37202793, -3.40040416, -3.40676289, -3.37124866,\n",
       "         -3.38897835, -3.33832317, -3.36753324, -3.34843485, -3.36678836,\n",
       "         -3.38828043, -3.38592799, -3.37889129, -3.38237598, -3.40485849,\n",
       "         -3.36520207, -3.39295312, -3.37470367, -3.3796353 , -3.36949586,\n",
       "         -3.35355273, -3.38138456, -3.36168177, -3.35752646, -3.39325322,\n",
       "         -3.35559973, -3.39039454, -3.38906176, -3.37315856, -3.39075283,\n",
       "         -3.39118078, -3.3877891 , -3.38311047, -3.37963096, -3.36790271,\n",
       "         -3.38677077, -3.376067  , -3.36194473, -3.37208702, -3.36301926,\n",
       "         -3.3783089 , -3.38181798, -3.37222581, -3.39293204, -3.39545714,\n",
       "         -3.336764  , -3.3698699 , -3.36448945, -3.35906173, -3.38804793,\n",
       "         -3.37668551, -3.37874335, -3.37997345, -3.37656357, -3.361764  ,\n",
       "         -3.37917641, -3.35675203, -3.37465375, -3.36953731, -3.40096849,\n",
       "         -3.36421898, -3.38540647, -3.402791  , -3.39162574, -3.37684015,\n",
       "         -3.37384574, -3.39140965, -3.36047516, -3.37427547, -3.38117866,\n",
       "         -3.38043216, -3.39107011, -3.39953459, -3.36934977, -3.38528269,\n",
       "         -3.37729689, -3.38114053, -3.36094229, -3.35288812, -3.36891098,\n",
       "         -3.35522623, -3.39738937, -3.37820363, -3.38474173, -3.36855954]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-3.37797188],\n",
       "        [ 3.85710973],\n",
       "        [ 4.44536904],\n",
       "        [-1.50116435],\n",
       "        [ 4.31270896],\n",
       "        [ 2.431996  ],\n",
       "        [-1.97924496],\n",
       "        [ 1.09658573],\n",
       "        [ 1.65301503],\n",
       "        [-5.16004716]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 拟合结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x,w):\n",
    "    data_y= 1/(1+np.exp(-np.dot(data_x,w)))\n",
    "    return (data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.matrix(np.zeros(data_x.shape[0]).reshape(data_x.shape[0],1))\n",
    "for i in range(0,w_hat.shape[1]):\n",
    "    pred_y = pred_y + logistic(data_x,w_hat[:,i])\n",
    "pred_y = pred_y / w.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = np.zeros(1000)\n",
    "for i in range(pred_y.shape[0]):\n",
    "    if (pred_y[i]>0.5):\n",
    "        pred_label[i] = 1\n",
    "    else:\n",
    "        pred_label[i] = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率为：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.933"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"准确率为：\")\n",
    "sum(pred_label == y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测精度为：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.250727303443535"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"预测精度为：\")\n",
    "np.sqrt(np.mean(np.array(pred_y - (np.matrix(y_prob).T))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 方法比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49790228825017463"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([-0.0778035 ,  0.07916073,  0.09882881, -0.02792082,  0.08068515,\n",
       "        0.06042243, -0.03774281,  0.01563705,  0.02977495, -0.10106615])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "均方根误差:0.26\n",
      "准确率为：\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.969"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model        #表示，可以调用sklearn中的linear_model模块进行线性回归。\n",
    "import numpy as np\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(data_x, y_prob)\n",
    "display(model.intercept_)  #截距\n",
    "display(model.coef_)  #线性模型的系数\n",
    "pred = model.predict(data_x)\n",
    "print('均方根误差:{:.2f}'.format(np.sqrt(np.mean((pred - y_prob) ** 2))))\n",
    "pred_ols_label = np.zeros(1000)\n",
    "for i in range(pred.shape[0]):\n",
    "    if (pred[i]>0.5):\n",
    "        pred_ols_label[i] = 1\n",
    "    else:\n",
    "        pred_ols_label[i] = 0\n",
    "print(\"准确率为：\")\n",
    "sum(pred_ols_label == y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.22960699,  1.00590698,  1.22657956, -0.64898397,  1.14053917,\n",
       "        0.55696027, -0.80901651,  0.16339276,  0.36252614, -1.7682974 ])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(w-np.mean(w))/np.std(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19361203, 0.30090584, 0.31981479, 0.00445859, 0.11385817])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[10:15]-y_prob[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.57740486e-05, 1.58600395e-04, 1.24320914e-03, 2.00473944e-09,\n",
       "       1.31219291e-07])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.66068289e-06],\n",
       "       [ 1.74428411e-04],\n",
       "       [ 2.42494690e-03],\n",
       "       [-1.75636756e-09],\n",
       "       [-1.03795728e-07]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_y - (np.matrix(y_prob).T))[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R值(准确率): 0.999\n",
      "参数: [[-11.57277267  12.96404414  15.44018899  -5.25203349  14.79914255\n",
      "    8.1335787   -6.87741363   3.63224953   5.87521227 -17.86338241]]\n",
      "截距: [-0.36399168]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV,LinearRegression\n",
    "lr = LogisticRegressionCV(multi_class=\"ovr\",fit_intercept=True,Cs=np.logspace(-2,2,20),cv=2,penalty=\"l2\",solver=\"lbfgs\",tol=0.01)\n",
    "re = lr.fit(data_x,y)\n",
    "r = re.score(data_x,y)\n",
    "print(\"R值(准确率):\",r)\n",
    "print(\"参数:\",re.coef_)\n",
    "print(\"截距:\",re.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 MQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 为什么用线性回归预测出来的参数与原参数不同（是否经过了标准化？？）\n",
    "- VB的效果在不同的样本中表现的差异性较大，因此造成了精度的下降\n",
    "- future： 因为现在的w是由一个正态分布生成的，但在现实生活中不同特征的数值服从的正态分布往往不同，因此效果可能会更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 重新生成$w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_w = range(0,p)\n",
    "center_w = range(0,p)\n",
    "n_w = 100\n",
    "w_temp = np.matrix(np.zeros(n_w*p).reshape(n_w,p))\n",
    "for i in range(0,p):\n",
    "    w_temp[:,i] = np.matrix((np.random.randn(n_w)*np.sqrt(var_w[i])+center_w[i]).reshape(n_w,1))\n",
    "w_new = np.array(np.mean(w_temp, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.01765953, 2.07273947, 3.04665257, 4.1166877 ,\n",
       "        4.71919111, 6.19202935, 7.22592054, 8.1422192 , 9.20451776]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00]\n",
      " [7.74294483e-13]\n",
      " [1.00000000e+00]\n",
      " [9.99997755e-01]\n",
      " [2.52057224e-23]\n",
      " [1.00000000e+00]\n",
      " [9.96849692e-01]\n",
      " [9.91332305e-01]\n",
      " [9.99999999e-01]]\n"
     ]
    }
   ],
   "source": [
    "global data_y\n",
    "data_y= 1/(1+np.exp(-np.dot(data_x,w_new.T)))\n",
    "print(data_y[1:10])\n",
    "y_prob = data_y\n",
    "#maxprob = max(y_prob)\n",
    "global y\n",
    "y = np.zeros(1000)\n",
    "for i in range(y_prob.shape[0]):\n",
    "    if (y_prob[i]>0.5):\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.957740391461023e-06"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-ff23119865b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randn\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.standard_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.cont0_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "np.random.randn(p)*np.sqrt(var)+center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-dd913fa20f22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcenter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_new\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0minitial_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcenter\u001b[0m\u001b[1;31m####利用w的先验随机生成w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mw0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.randn\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.standard_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.cont0_array\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "eta = 0.01####学习率\n",
    "var = np.var(w_new)\n",
    "center = np.mean(w_new)\n",
    "initial_w = np.random.randn(p)*np.sqrt(var)+center####利用w的先验随机生成w\n",
    "w0 = initial_w\n",
    "epsilon = 1\n",
    "i = 0\n",
    "rec = []\n",
    "while(epsilon>0.001):\n",
    "    w1 = w0 + eta * delta1(w0)\n",
    "    epsilon = abs(cal(w1) - cal(w0))\n",
    "    rec.append(epsilon) \n",
    "    i = i+1\n",
    "    w0 = w1\n",
    "    print(epsilon)\n",
    "mu = w0\n",
    "v = -delta2(w0)\n",
    "sigma = 1/v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. 求解KL散度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\n",
    "KL = E[log\\frac{g(w;\\mu,\\Sigma)}{p(w|X,y)}] \n",
    "= E[log\\frac{g(w;\\mu,\\Sigma)}{\\frac{p(y|X,w)p(w)}{p(y|x)}}]\n",
    "= E[log\\frac{g(w;\\mu,\\Sigma)}{p(y|X,w)p(w)}+logp(y|X)]\n",
    "= E[logg(w;\\mu,\\Sigma) - logp(y|X,w) - logp(w)] +logp(y|X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdfcal(w,mu,var):\n",
    "    w = W.tolist()\n",
    "    n = len(w)\n",
    "    cdf = 1\n",
    "    for i in range(0,n):\n",
    "        cdf = cdf * stats.norm.cdf(w[i], mu[i], var[i])\n",
    "    return (cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "logpy = 0;w  = w0;K = cal(w);cdf = 1;KL = 0\n",
    "wh = np.matrix(np.zeros(w.shape[0]*100).reshape(w.shape[0],100))\n",
    "for j in range(0,w.shape[0]):\n",
    "    np.linspace(-4,4,100)\n",
    "    mu1 = w[j] \n",
    "    sigma1 = sigma[j]\n",
    "    lower = mu1 - 4*sigma1\n",
    "    upper = mu1 + 4*sigma1\n",
    "    wh[j,] = np.linspace(lower,upper,100)\n",
    "for index in range(wh.shape[1]):\n",
    "    W = wh[0:w.shape[0],index]\n",
    "    for i in range(0,data_x.shape[0]):\n",
    "        logpy = logpy+np.log(1/(1+np.exp(-np.dot(data_x[i,],W))))\n",
    "    KL = KL+(cal(W) - logpy)*cdfcal(W,w,sigma)\n",
    "w3 = np.linspace(-4,4,100)\n",
    "Elogp = 0\n",
    "for k in range(0,len(w3)):\n",
    "    Elogp = Elogp + stats.norm.cdf(w3[k], center, np.sqrt(var))*np.log(stats.norm.cdf(w3[k], center, np.sqrt(var)))\n",
    "#KL - Elogp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19385210.03323192"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KL[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3 = np.linspace(-4,4,100)\n",
    "Elogp = 0\n",
    "for k in range(0,len(w3)):\n",
    "    Elogp = Elogp + stats.norm.cdf(w3[k], center, np.sqrt(var))*np.log(stats.norm.pdf(w3[k], center, np.sqrt(var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-3.32027012],\n",
       "        [ 3.9580035 ],\n",
       "        [ 4.51801704],\n",
       "        [-1.4725416 ],\n",
       "        [ 4.38836399],\n",
       "        [ 2.51719495],\n",
       "        [-1.92168556],\n",
       "        [ 1.1639101 ],\n",
       "        [ 1.71652767],\n",
       "        [-5.0712452 ]])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdfcal(W,w,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = np.matrix(np.zeros(w.shape[0]*100).reshape(w.shape[0],100))\n",
    "for j in range(w.shape[0]):\n",
    "    mu1 = w[j] \n",
    "    sigma1 = sigma[j]\n",
    "    lower = mu1 - 4*sigma1\n",
    "    upper = mu1 + 4*sigma1\n",
    "    wh[j,] = np.linspace(lower,upper,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.0551819045977\n"
     ]
    }
   ],
   "source": [
    "mu = w0\n",
    "v = delta2(w0)\n",
    "sigma = 1/v\n",
    "logpy = 0\n",
    "w  = w0\n",
    "K = cal(w)\n",
    "wh = np.zeros(10)\n",
    "for j in range(0,w.shape[0]):\n",
    "    mu1 = w0 \n",
    "    sigma1 = sigma[j]\n",
    "    wh[j] = (np.random.randn(1)*np.sqrt(sigma1)+mu1[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.92342123e-01, 4.59247259e-02, 1.70683827e-02, 3.73888345e-01,\n",
       "       2.39269131e-01, 2.59415310e-01, 2.72359885e-01, 3.37361546e-03,\n",
       "       3.74467024e-05, 4.05817482e-03])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wh-mu)*(wh-mu)/(2*sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.06548327, 8.91190067, 8.94075701, 8.58393705, 8.71855626,\n",
       "       8.69841008, 8.68546551, 8.95445178, 8.95778794, 8.95376722])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(K)-(wh-mu)*(wh-mu)/(2*sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(sigma1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. GaussianMixture from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "color_iter = itertools.cycle(['navy', 'c', 'cornflowerblue', 'gold',\n",
    "                              'darkorange'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(X, Y_, means, covariances, index, title):\n",
    "    splot = plt.subplot(2, 1, 1 + index)\n",
    "    for i, (mean, covar, color) in enumerate(zip(\n",
    "            means, covariances, color_iter)):\n",
    "        v, w = linalg.eigh(covar)\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])\n",
    "        # as the DP will not use every component it has access to\n",
    "        # unless it needs it, we shouldn't plot the redundant\n",
    "        # components.\n",
    "        if not np.any(Y_ == i):\n",
    "            continue\n",
    "        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "        # Plot an ellipse to show the Gaussian component\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        splot.add_artist(ell)\n",
    "\n",
    "    plt.xlim(-9., 5.)\n",
    "    plt.ylim(-3., 6.)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Number of samples per component\n",
    "n_samples = 500\n",
    "\n",
    "# Generate random sample, two components\n",
    "np.random.seed(0)\n",
    "C = np.array([[0., -0.1], [1.7, .4]])\n",
    "X = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n",
    "          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a Gaussian mixture with EM using five components\n",
    "gmm = mixture.GaussianMixture(n_components=5, covariance_type='full').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD7CAYAAABdXO4CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwFP++QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24e03e5ccf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0,\n",
    "             'Gaussian Mixture')\n",
    "# Fit a Dirichlet process Gaussian mixture using five components\n",
    "dpgmm = mixture.BayesianGaussianMixture(n_components=5,\n",
    "                                        covariance_type='full').fit(X)\n",
    "plot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,\n",
    "             'Bayesian Gaussian Mixture with a Dirichlet process prior')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -5.97878326e+00,   3.00076669e+00],\n",
       "       [ -3.04824425e+00,   1.51571998e+00],\n",
       "       [ -3.04816872e+00,   1.51568575e+00],\n",
       "       [ -3.04841758e+00,   1.51580487e+00],\n",
       "       [ -4.86789592e-02,  -5.20790518e-04]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpgmm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.51401566,  -4.13489296],\n",
       "       [ -4.13489296,   2.57580056]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpgmm.covariance_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.01371312,  1.50010815])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpgmm.mean_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
